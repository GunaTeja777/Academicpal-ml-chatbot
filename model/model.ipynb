{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = pd.read_csv(r'C:\\Users\\tejag\\OneDrive\\Desktop\\A chat bot\\Academicpal-ml-chatbot\\model\\Academicpal chatbot1styear - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Semester</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Notes Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st</td>\n",
       "      <td>Physics</td>\n",
       "      <td>New Physics</td>\n",
       "      <td>physics, science, mechanics</td>\n",
       "      <td>https://drive.google.com/drive/folders/1e-LQMg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Problem Solving</td>\n",
       "      <td>problem solving, psp, logic</td>\n",
       "      <td>https://drive.google.com/drive/u/5/folders/1yK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Maths</td>\n",
       "      <td>math, maths, calculus, algebra</td>\n",
       "      <td>https://drive.google.com/drive/folders/1sYdBua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1st</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Basic Electronics</td>\n",
       "      <td>electronics, basic electronics, circuits</td>\n",
       "      <td>https://drive.google.com/drive/folders/17iJtHY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>cyber security, security, hacking</td>\n",
       "      <td>https://drive.google.com/drive/folders/17kg_R1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Semester    Cycle            Subject  \\\n",
       "0      1st  Physics        New Physics   \n",
       "1      1st  Physics    Problem Solving   \n",
       "2      1st  Physics              Maths   \n",
       "3      1st  Physics  Basic Electronics   \n",
       "4      1st  Physics     Cyber Security   \n",
       "\n",
       "                                   Keywords  \\\n",
       "0               physics, science, mechanics   \n",
       "1               problem solving, psp, logic   \n",
       "2            math, maths, calculus, algebra   \n",
       "3  electronics, basic electronics, circuits   \n",
       "4         cyber security, security, hacking   \n",
       "\n",
       "                                          Notes Link  \n",
       "0  https://drive.google.com/drive/folders/1e-LQMg...  \n",
       "1  https://drive.google.com/drive/u/5/folders/1yK...  \n",
       "2  https://drive.google.com/drive/folders/1sYdBua...  \n",
       "3  https://drive.google.com/drive/folders/17iJtHY...  \n",
       "4  https://drive.google.com/drive/folders/17kg_R1...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Semester      0\n",
       "Cycle         0\n",
       "Subject       0\n",
       "Keywords      0\n",
       "Notes Link    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Semester\n",
       "1st    20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[\"Semester\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.18.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: click in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejag\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk fuzzywuzzy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tejag\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tejag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tejag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Download NLTK data (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"Academicpal chatbot1styear - Sheet1.csv\"  # Update with actual path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert to lowercase for better matching\n",
    "df[\"Subject\"] = df[\"Subject\"].str.lower()\n",
    "df[\"Cycle\"] = df[\"Cycle\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"Academicpal chatbot1styear - Sheet1.csv\"  # Update with actual path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert to lowercase for better matching\n",
    "df[\"Subject\"] = df[\"Subject\"].str.lower()\n",
    "df[\"Cycle\"] = df[\"Cycle\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize & lowercase\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cycles: ['physics' 'chemistry']\n",
      "Here is the link to your maths (physics) notes: https://drive.google.com/drive/folders/1sYdBua6wr7uhMYw4uIKw5hrPtYd10c_B\n"
     ]
    }
   ],
   "source": [
    "def get_notes(df):\n",
    "    user_input = input(\"What notes do you need? \")  # Example: \"I want maths notes\"\n",
    "    processed_input = preprocess_input(user_input)  \n",
    "\n",
    "    # Use fuzzy matching to find the best subject\n",
    "    subject_list = df[\"Subject\"].unique()\n",
    "    best_match, score = process.extractOne(processed_input, subject_list)\n",
    "\n",
    "    if score < 50:  # If confidence is low\n",
    "        print(\"No relevant subject found. Please try again.\")\n",
    "        return\n",
    "\n",
    "    # Filter subjects that match\n",
    "    matched_subjects = df[df[\"Subject\"] == best_match]\n",
    "\n",
    "    # Check if multiple cycles exist\n",
    "    unique_cycles = matched_subjects[\"Cycle\"].unique()\n",
    "\n",
    "    if len(unique_cycles) > 1:\n",
    "        print(\"Available cycles:\", unique_cycles)\n",
    "        cycle = input(\"Enter the cycle: \").strip().lower()\n",
    "\n",
    "        # Filter by cycle\n",
    "        matched_subjects = matched_subjects[matched_subjects[\"Cycle\"] == cycle]\n",
    "\n",
    "    # Get the notes link\n",
    "    if not matched_subjects.empty:\n",
    "        link = matched_subjects[\"Notes Link\"].values[0]\n",
    "        print(f\"Here is the link to your {best_match} ({cycle}) notes: {link}\")\n",
    "    else:\n",
    "        print(\"No notes found for the given subject and cycle.\")\n",
    "get_notes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tejag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tejag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tejag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Academicpal chatbot1styear - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([word for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed_Subject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mpreprocess_text\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"Processed_Subject\"] = df[\"Subject\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"Processed_Subject\"])\n",
    "y = df[\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"subject_predictor.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
